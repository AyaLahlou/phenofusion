{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Driver Data Extraction - Step by Step with Diagnostics\n",
    "\n",
    "This notebook walks through the climate driver importance extraction process from TFT model predictions, with detailed diagnostics at each step.\n",
    "\n",
    "Based on: `/burg-archive/home/al4385/phenofusion/src/phenofusion/dataio/driversdata.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.stats import linregress\n",
    "from typing import List, Tuple, Optional\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Step 2: Configure Parameters\n",
    "\n",
    "Set the file paths and parameters for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "PFT = \"SHR\"  # Plant functional type\n",
    "FORECAST_WINDOW = 30  # Days\n",
    "\n",
    "# File paths\n",
    "DATA_PATH = (\n",
    "    f\"/burg/glab/users/al4385/data/TFT_40_overlapping_samples/{PFT}_1982_2021.pkl\"\n",
    ")\n",
    "PRED_PATH = (\n",
    "    f\"/burg/glab/users/al4385/predictions/pred_40year_moresamples/{PFT}_20251023.pkl\"\n",
    ")\n",
    "COORD_PATH = f\"/burg/glab/users/al4385/data/coordinates/{PFT}.parquet\"\n",
    "OUTPUT_BASE = (\n",
    "    f\"/burg/glab/users/ms7073/analysis/driversdata/oversampling/{PFT}_diagnostic\"\n",
    ")\n",
    "\n",
    "# Phenology detection thresholds\n",
    "MIN_DIFF = 0.01 if PFT in [\"BET\", \"SHR\"] else 0.20\n",
    "MIN_SLOPE = 0.001 if PFT in [\"BET\", \"SHR\"] else 0.002\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  PFT: {PFT}\")\n",
    "print(f\"  Forecast Window: {FORECAST_WINDOW} days\")\n",
    "print(f\"  Min CSIF Difference: {MIN_DIFF}\")\n",
    "print(f\"  Min Slope: {MIN_SLOPE}\")\n",
    "print(f\"\\nFile paths:\")\n",
    "print(f\"  Data: {DATA_PATH}\")\n",
    "print(f\"  Predictions: {PRED_PATH}\")\n",
    "print(f\"  Coordinates: {COORD_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Step 3: Load Data Files\n",
    "\n",
    "Load the processed data, predictions, and coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original data\n",
    "print(\"Loading data...\")\n",
    "with open(DATA_PATH, \"rb\") as fp:\n",
    "    data = pickle.load(fp)\n",
    "\n",
    "print(f\"✓ Data loaded\")\n",
    "print(f\"  Keys: {list(data.keys())}\")\n",
    "print(f\"  Data sets: {list(data['data_sets'].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions\n",
    "print(\"Loading predictions...\")\n",
    "with open(PRED_PATH, \"rb\") as fp:\n",
    "    preds = pickle.load(fp)\n",
    "\n",
    "print(f\"✓ Predictions loaded\")\n",
    "print(f\"  Keys: {list(preds.keys())}\")\n",
    "print(f\"  Attention scores shape: {preds['attention_scores'].shape}\")\n",
    "print(f\"  Predicted quantiles shape: {preds['predicted_quantiles'].shape}\")\n",
    "print(\n",
    "    f\"  Historical selection weights shape: {preds['historical_selection_weights'].shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load coordinates\n",
    "print(\"Loading coordinates...\")\n",
    "coords = pd.read_parquet(COORD_PATH).drop_duplicates()\n",
    "\n",
    "print(f\"✓ Coordinates loaded\")\n",
    "print(f\"  Shape: {coords.shape}\")\n",
    "print(f\"  Columns: {list(coords.columns)}\")\n",
    "print(f\"  Unique locations: {coords['location'].nunique()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "coords.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Step 4: Examine Test Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract test data\n",
    "test_data = data[\"data_sets\"][\"test\"]\n",
    "\n",
    "print(\"Test data structure:\")\n",
    "print(f\"  Keys: {list(test_data.keys())}\")\n",
    "print(f\"\\nShapes:\")\n",
    "for key, value in test_data.items():\n",
    "    if hasattr(value, \"shape\"):\n",
    "        print(f\"  {key}: {value.shape}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(value)}\")\n",
    "\n",
    "print(f\"\\nFirst few IDs:\")\n",
    "print(test_data[\"id\"][:5].flatten())\n",
    "\n",
    "print(f\"\\nFirst few target values (CSIF):\")\n",
    "print(test_data[\"target\"][:5].flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Step 5: Create Analysis DataFrame\n",
    "\n",
    "Combine data, predictions, and coordinates into a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base DataFrame\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Index\": test_data[\"id\"].flatten(),\n",
    "        \"CSIF\": test_data[\"target\"].flatten(),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add predictions (median/0.5 quantile)\n",
    "df[\"pred_05\"] = preds[\"predicted_quantiles\"][:, :, 1].flatten()\n",
    "\n",
    "print(f\"Base DataFrame created:\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Columns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse location and time from Index\n",
    "df[[\"location\", \"time\"]] = df[\"Index\"].str.split(\"_\", n=1, expand=True)\n",
    "df[\"location\"] = df[\"location\"].astype(int)\n",
    "df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "\n",
    "# Sort by location and time\n",
    "df = df.sort_values(by=[\"location\", \"time\"])\n",
    "\n",
    "# Add temporal features\n",
    "df[\"doy\"] = df[\"time\"].dt.dayofyear\n",
    "df[\"year\"] = df[\"time\"].dt.year\n",
    "df[\"month\"] = df[\"time\"].dt.month\n",
    "df[\"day\"] = df[\"time\"].dt.day\n",
    "\n",
    "# Drop index column\n",
    "df = df.drop(columns=[\"Index\"])\n",
    "\n",
    "print(f\"After parsing temporal information:\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Columns: {list(df.columns)}\")\n",
    "print(f\"  Date range: {df['time'].min()} to {df['time'].max()}\")\n",
    "print(f\"  Year range: {df['year'].min()} to {df['year'].max()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with coordinates\n",
    "df = pd.merge(coords, df, on=\"location\", how=\"left\")\n",
    "\n",
    "print(f\"After merging with coordinates:\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Columns: {list(df.columns)}\")\n",
    "print(f\"  Unique locations: {df['location'].nunique()}\")\n",
    "print(f\"  Records per location (avg): {len(df) / df['location'].nunique():.1f}\")\n",
    "print(f\"\\nSpatial extent:\")\n",
    "print(f\"  Latitude range: {df['latitude'].min():.2f} to {df['latitude'].max():.2f}\")\n",
    "print(f\"  Longitude range: {df['longitude'].min():.2f} to {df['longitude'].max():.2f}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Step 6: Visualize CSIF Time Series Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CSIF time series for a few random locations\n",
    "sample_locations = df[\"location\"].unique()[:4]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, loc in enumerate(sample_locations):\n",
    "    loc_data = df[df[\"location\"] == loc].sort_values(\"time\")\n",
    "\n",
    "    axes[idx].plot(loc_data[\"time\"], loc_data[\"CSIF\"], label=\"Observed\", alpha=0.7)\n",
    "    axes[idx].plot(loc_data[\"time\"], loc_data[\"pred_05\"], label=\"Predicted\", alpha=0.7)\n",
    "    axes[idx].set_title(\n",
    "        f'Location {loc}\\n(Lat: {loc_data[\"latitude\"].iloc[0]:.2f}, Lon: {loc_data[\"longitude\"].iloc[0]:.2f})'\n",
    "    )\n",
    "    axes[idx].set_xlabel(\"Date\")\n",
    "    axes[idx].set_ylabel(\"CSIF\")\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Sample CSIF time series plotted for 4 locations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Step 7: Detect Phenology Indices (SOS and EOS)\n",
    "\n",
    "Use CSIF slope analysis to identify Start of Season (SOS) and End of Season (EOS) samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists for phenology indices\n",
    "SOS_indices = []\n",
    "EOS_indices = []\n",
    "slopes_sos = []  # Store slopes for diagnostics\n",
    "slopes_eos = []\n",
    "csif_ranges_sos = []\n",
    "csif_ranges_eos = []\n",
    "\n",
    "batch_size = FORECAST_WINDOW\n",
    "sos_count = 0\n",
    "eos_count = 0\n",
    "skipped_incomplete = 0\n",
    "skipped_multi_location = 0\n",
    "skipped_low_signal = 0\n",
    "skipped_nan = 0\n",
    "\n",
    "\n",
    "print(f\"Scanning {len(df)} records in batches of {batch_size}...\\n\")\n",
    "\n",
    "for start in range(0, len(df), batch_size):\n",
    "    batch_df = df.iloc[start : start + batch_size]\n",
    "\n",
    "    if batch_df[\"CSIF\"].isna().any():\n",
    "        skipped_nan += 1\n",
    "        continue\n",
    "    # Skip incomplete batches\n",
    "    if len(batch_df) < batch_size:\n",
    "        skipped_incomplete += 1\n",
    "        continue\n",
    "\n",
    "    # Check if batch is from same location\n",
    "    if batch_df[\"location\"].nunique() > 1:\n",
    "        skipped_multi_location += 1\n",
    "        continue\n",
    "\n",
    "    # Get CSIF values\n",
    "    csif_values = batch_df[\"CSIF\"].values\n",
    "\n",
    "    # Check if there's sufficient signal\n",
    "    csif_range = abs(csif_values[-1] - csif_values[0])\n",
    "    if csif_range < MIN_DIFF:\n",
    "        skipped_low_signal += 1\n",
    "        continue\n",
    "\n",
    "    # Calculate slope\n",
    "    x = np.arange(len(csif_values))\n",
    "    slope, _, _, _, _ = linregress(x, csif_values)\n",
    "\n",
    "    # Classify as SOS or EOS based on slope\n",
    "    if slope >= MIN_SLOPE:\n",
    "        # Positive slope = Start of Season\n",
    "        SOS_indices.append(batch_df.index[0])\n",
    "        slopes_sos.append(slope)\n",
    "        csif_ranges_sos.append(csif_range)\n",
    "        sos_count += 1\n",
    "    elif slope <= -MIN_SLOPE - 0.0005:\n",
    "        # Negative slope = End of Season\n",
    "        EOS_indices.append(batch_df.index[0])\n",
    "        slopes_eos.append(slope)\n",
    "        csif_ranges_eos.append(csif_range)\n",
    "        eos_count += 1\n",
    "\n",
    "print(f\"Phenology Detection Results:\")\n",
    "print(f\"  Total batches processed: {len(df) // batch_size}\")\n",
    "print(f\"  Skipped (incomplete): {skipped_incomplete}\")\n",
    "print(f\"  Skipped (multi-location): {skipped_multi_location}\")\n",
    "print(f\"  Skipped (low signal): {skipped_low_signal}\")\n",
    "print(f\"\\n  SOS samples detected: {sos_count}\")\n",
    "print(f\"  EOS samples detected: {eos_count}\")\n",
    "print(f\"  Skipped (NaNs): {skipped_nan}\")\n",
    "\n",
    "if sos_count > 0:\n",
    "    print(\n",
    "        f\"\\n  SOS slopes: min={min(slopes_sos):.6f}, max={max(slopes_sos):.6f}, mean={np.mean(slopes_sos):.6f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  SOS CSIF ranges: min={min(csif_ranges_sos):.4f}, max={max(csif_ranges_sos):.4f}, mean={np.mean(csif_ranges_sos):.4f}\"\n",
    "    )\n",
    "\n",
    "if eos_count > 0:\n",
    "    print(\n",
    "        f\"\\n  EOS slopes: min={min(slopes_eos):.6f}, max={max(slopes_eos):.6f}, mean={np.mean(slopes_eos):.6f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  EOS CSIF ranges: min={min(csif_ranges_eos):.4f}, max={max(csif_ranges_eos):.4f}, mean={np.mean(csif_ranges_eos):.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"  Skipped (NaNs): {skipped_nan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to prediction indices\n",
    "SOS_pred_indices = [int(i / batch_size) for i in SOS_indices]\n",
    "EOS_pred_indices = [int(i / batch_size) for i in EOS_indices]\n",
    "\n",
    "# Filter out indices beyond prediction array bounds\n",
    "max_pred_index = len(preds[\"attention_scores\"]) - 1\n",
    "SOS_pred_indices = [idx for idx in SOS_pred_indices if idx <= max_pred_index]\n",
    "EOS_pred_indices = [idx for idx in EOS_pred_indices if idx <= max_pred_index]\n",
    "\n",
    "print(f\"Prediction Indices:\")\n",
    "print(f\"  SOS prediction indices: {len(SOS_pred_indices)} (filtered from {sos_count})\")\n",
    "print(f\"  EOS prediction indices: {len(EOS_pred_indices)} (filtered from {eos_count})\")\n",
    "print(f\"  Max valid prediction index: {max_pred_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Step 8: Visualize Slope Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "if len(slopes_sos) > 0:\n",
    "    axes[0].hist(slopes_sos, bins=50, alpha=0.7, color=\"green\", edgecolor=\"black\")\n",
    "    axes[0].axvline(\n",
    "        MIN_SLOPE, color=\"red\", linestyle=\"--\", label=f\"Threshold: {MIN_SLOPE}\"\n",
    "    )\n",
    "    axes[0].set_title(f\"SOS Slope Distribution (n={len(slopes_sos)})\")\n",
    "    axes[0].set_xlabel(\"Slope\")\n",
    "    axes[0].set_ylabel(\"Frequency\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "if len(slopes_eos) > 0:\n",
    "    axes[1].hist(slopes_eos, bins=50, alpha=0.7, color=\"brown\", edgecolor=\"black\")\n",
    "    axes[1].axvline(\n",
    "        -MIN_SLOPE - 0.0005,\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Threshold: {-MIN_SLOPE - 0.0005:.4f}\",\n",
    "    )\n",
    "    axes[1].set_title(f\"EOS Slope Distribution (n={len(slopes_eos)})\")\n",
    "    axes[1].set_xlabel(\"Slope\")\n",
    "    axes[1].set_ylabel(\"Frequency\")\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Step 9: Visualize Example SOS and EOS Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot example SOS and EOS windows\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 2 SOS examples\n",
    "if len(SOS_indices) >= 2:\n",
    "    for i in range(2):\n",
    "        idx = SOS_indices[i]\n",
    "        window = df.iloc[idx : idx + batch_size]\n",
    "        axes[0, i].plot(window[\"doy\"], window[\"CSIF\"], \"go-\", label=\"CSIF\")\n",
    "        axes[0, i].set_title(f\"SOS Example {i+1}\\nSlope: {slopes_sos[i]:.6f}\")\n",
    "        axes[0, i].set_xlabel(\"Day of Year\")\n",
    "        axes[0, i].set_ylabel(\"CSIF\")\n",
    "        axes[0, i].legend()\n",
    "        axes[0, i].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2 EOS examples\n",
    "if len(EOS_indices) >= 2:\n",
    "    for i in range(2):\n",
    "        idx = EOS_indices[i]\n",
    "        window = df.iloc[idx : idx + batch_size]\n",
    "        axes[1, i].plot(window[\"doy\"], window[\"CSIF\"], \"ro-\", label=\"CSIF\")\n",
    "        axes[1, i].set_title(f\"EOS Example {i+1}\\nSlope: {slopes_eos[i]:.6f}\")\n",
    "        axes[1, i].set_xlabel(\"Day of Year\")\n",
    "        axes[1, i].set_ylabel(\"CSIF\")\n",
    "        axes[1, i].legend()\n",
    "        axes[1, i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Step 10: Extract Driver Weights for SOS Samples\n",
    "\n",
    "For each SOS sample, find the time window with maximum attention and extract driver importance weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_attention_window(index: int, preds, forecast_window: int) -> int:\n",
    "    \"\"\"\n",
    "    Find the time window with maximum attention scores.\n",
    "    \"\"\"\n",
    "    # Get mean attention across all horizons\n",
    "    att_array = np.mean(preds[\"attention_scores\"][index], axis=0)\n",
    "\n",
    "    max_sum = -np.inf\n",
    "    best_start_index = None\n",
    "\n",
    "    # Slide window to find maximum\n",
    "    # max_start = len(att_array) - forecast_window\n",
    "    max_start = 336\n",
    "    for i in range(max_start):\n",
    "        current_sum = np.sum(att_array[i : i + forecast_window])\n",
    "        if current_sum > max_sum:\n",
    "            max_sum = current_sum\n",
    "            best_start_index = i\n",
    "\n",
    "    return best_start_index if best_start_index is not None else 0\n",
    "\n",
    "\n",
    "print(\"Extracting SOS driver weights...\")\n",
    "\n",
    "sos_driver_data = []\n",
    "max_window_start = 365 - FORECAST_WINDOW\n",
    "skipped_window = 0\n",
    "processed = 0\n",
    "\n",
    "for index in SOS_pred_indices[:5]:  # Show diagnostics for first 5\n",
    "    try:\n",
    "        # Find maximum attention window\n",
    "        window_start = find_max_attention_window(index, preds, FORECAST_WINDOW)\n",
    "\n",
    "        print(f\"\\nSample {index}:\")\n",
    "        print(f\"  Max attention window starts at day: {window_start}\")\n",
    "\n",
    "        # Skip if window extends beyond valid range\n",
    "        if window_start > max_window_start:\n",
    "            print(f\"  Skipped: window extends beyond valid range\")\n",
    "            skipped_window += 1\n",
    "            continue\n",
    "\n",
    "        # Extract median weights for each driver\n",
    "        weights = {}\n",
    "        hist_weights = preds[\"historical_selection_weights\"][index]\n",
    "\n",
    "        driver_names = [\"tmin\", \"tmax\", \"rad\", \"precip\", \"photo\", \"sm\"]\n",
    "        for i, var in enumerate(driver_names, 1):\n",
    "            weight_values = hist_weights[\n",
    "                window_start : window_start + FORECAST_WINDOW, i\n",
    "            ]\n",
    "            weights[f\"hist_{var}\"] = np.median(weight_values)\n",
    "            print(f\"  {var}: {weights[f'hist_{var}']:.4f}\")\n",
    "\n",
    "        # Get location ID\n",
    "        location_id = int(data[\"data_sets\"][\"test\"][\"id\"][index][0].split(\"_\")[0])\n",
    "        weights[\"location\"] = location_id\n",
    "        print(f\"  Location: {location_id}\")\n",
    "\n",
    "        sos_driver_data.append(weights)\n",
    "        processed += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing index {index}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nProcessed {processed} samples for diagnostics\")\n",
    "print(f\"Total SOS samples to process: {len(SOS_pred_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all SOS samples\n",
    "print(\"Processing all SOS samples...\")\n",
    "\n",
    "sos_driver_data = []\n",
    "skipped_window = 0\n",
    "errors = 0\n",
    "\n",
    "for index in SOS_pred_indices:\n",
    "    try:\n",
    "        window_start = find_max_attention_window(index, preds, FORECAST_WINDOW)\n",
    "\n",
    "        if window_start > max_window_start:\n",
    "            skipped_window += 1\n",
    "            continue\n",
    "\n",
    "        weights = {}\n",
    "        hist_weights = preds[\"historical_selection_weights\"][index]\n",
    "\n",
    "        for i, var in enumerate([\"tmin\", \"tmax\", \"rad\", \"precip\", \"photo\", \"sm\"], 1):\n",
    "            weights[f\"hist_{var}\"] = np.median(\n",
    "                hist_weights[window_start : window_start + FORECAST_WINDOW, i]\n",
    "            )\n",
    "\n",
    "        location_id = int(data[\"data_sets\"][\"test\"][\"id\"][index][0].split(\"_\")[0])\n",
    "        weights[\"location\"] = location_id\n",
    "\n",
    "        sos_driver_data.append(weights)\n",
    "\n",
    "    except Exception as e:\n",
    "        errors += 1\n",
    "        continue\n",
    "\n",
    "# Create DataFrame\n",
    "sos_driver_df = pd.DataFrame(sos_driver_data)\n",
    "\n",
    "print(f\"\\nSOS Driver Extraction Results:\")\n",
    "print(f\"  Input samples: {len(SOS_pred_indices)}\")\n",
    "print(f\"  Skipped (window): {skipped_window}\")\n",
    "print(f\"  Errors: {errors}\")\n",
    "print(f\"  Successfully extracted: {len(sos_driver_df)}\")\n",
    "print(f\"\\nDriver DataFrame shape: {sos_driver_df.shape}\")\n",
    "print(f\"Columns: {list(sos_driver_df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "sos_driver_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Step 11: Extract Driver Weights for EOS Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing all EOS samples...\")\n",
    "\n",
    "eos_driver_data = []\n",
    "skipped_window = 0\n",
    "errors = 0\n",
    "\n",
    "for index in EOS_pred_indices:\n",
    "    try:\n",
    "        window_start = find_max_attention_window(index, preds, FORECAST_WINDOW)\n",
    "\n",
    "        if window_start > max_window_start:\n",
    "            skipped_window += 1\n",
    "            continue\n",
    "\n",
    "        weights = {}\n",
    "        hist_weights = preds[\"historical_selection_weights\"][index]\n",
    "\n",
    "        for i, var in enumerate([\"tmin\", \"tmax\", \"rad\", \"precip\", \"photo\", \"sm\"], 1):\n",
    "            weights[f\"hist_{var}\"] = np.median(\n",
    "                hist_weights[window_start : window_start + FORECAST_WINDOW, i]\n",
    "            )\n",
    "\n",
    "        location_id = int(data[\"data_sets\"][\"test\"][\"id\"][index][0].split(\"_\")[0])\n",
    "        weights[\"location\"] = location_id\n",
    "\n",
    "        eos_driver_data.append(weights)\n",
    "\n",
    "    except Exception as e:\n",
    "        errors += 1\n",
    "        continue\n",
    "\n",
    "# Create DataFrame\n",
    "eos_driver_df = pd.DataFrame(eos_driver_data)\n",
    "\n",
    "print(f\"\\nEOS Driver Extraction Results:\")\n",
    "print(f\"  Input samples: {len(EOS_pred_indices)}\")\n",
    "print(f\"  Skipped (window): {skipped_window}\")\n",
    "print(f\"  Errors: {errors}\")\n",
    "print(f\"  Successfully extracted: {len(eos_driver_df)}\")\n",
    "print(f\"\\nDriver DataFrame shape: {eos_driver_df.shape}\")\n",
    "print(f\"Columns: {list(eos_driver_df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "eos_driver_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Step 12: Visualize Driver Weight Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot driver weight distributions for SOS\n",
    "if len(sos_driver_df) > 0:\n",
    "    driver_cols = [\n",
    "        \"hist_tmin\",\n",
    "        \"hist_tmax\",\n",
    "        \"hist_rad\",\n",
    "        \"hist_precip\",\n",
    "        \"hist_photo\",\n",
    "        \"hist_sm\",\n",
    "    ]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, col in enumerate(driver_cols):\n",
    "        axes[idx].hist(\n",
    "            sos_driver_df[col].dropna(),\n",
    "            bins=50,\n",
    "            alpha=0.7,\n",
    "            color=\"green\",\n",
    "            edgecolor=\"black\",\n",
    "        )\n",
    "        axes[idx].set_title(\n",
    "            f'SOS - {col.replace(\"hist_\", \"\").upper()}\\nMean: {sos_driver_df[col].mean():.4f}'\n",
    "        )\n",
    "        axes[idx].set_xlabel(\"Weight\")\n",
    "        axes[idx].set_ylabel(\"Frequency\")\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle(\"SOS Driver Weight Distributions\", fontsize=16, y=1.00)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Summary statistics\n",
    "    print(\"SOS Driver Weight Summary:\")\n",
    "    print(sos_driver_df[driver_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot driver weight distributions for EOS\n",
    "if len(eos_driver_df) > 0:\n",
    "    driver_cols = [\n",
    "        \"hist_tmin\",\n",
    "        \"hist_tmax\",\n",
    "        \"hist_rad\",\n",
    "        \"hist_precip\",\n",
    "        \"hist_photo\",\n",
    "        \"hist_sm\",\n",
    "    ]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, col in enumerate(driver_cols):\n",
    "        axes[idx].hist(\n",
    "            eos_driver_df[col].dropna(),\n",
    "            bins=50,\n",
    "            alpha=0.7,\n",
    "            color=\"brown\",\n",
    "            edgecolor=\"black\",\n",
    "        )\n",
    "        axes[idx].set_title(\n",
    "            f'EOS - {col.replace(\"hist_\", \"\").upper()}\\nMean: {eos_driver_df[col].mean():.4f}'\n",
    "        )\n",
    "        axes[idx].set_xlabel(\"Weight\")\n",
    "        axes[idx].set_ylabel(\"Frequency\")\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle(\"EOS Driver Weight Distributions\", fontsize=16, y=1.00)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Summary statistics\n",
    "    print(\"EOS Driver Weight Summary:\")\n",
    "    print(eos_driver_df[driver_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## Step 13: Merge with Coordinates and Check for Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge SOS drivers with coordinates\n",
    "sos_coord_driver_df = pd.merge(coords, sos_driver_df, on=\"location\", how=\"left\")\n",
    "\n",
    "print(\"SOS Data After Coordinate Merge:\")\n",
    "print(f\"  Total locations in coords: {len(coords)}\")\n",
    "print(f\"  Locations with SOS driver data: {len(sos_driver_df)}\")\n",
    "print(f\"  Final merged DataFrame: {sos_coord_driver_df.shape}\")\n",
    "\n",
    "driver_cols = [\n",
    "    \"hist_tmin\",\n",
    "    \"hist_tmax\",\n",
    "    \"hist_rad\",\n",
    "    \"hist_precip\",\n",
    "    \"hist_photo\",\n",
    "    \"hist_sm\",\n",
    "]\n",
    "missing_mask = sos_coord_driver_df[driver_cols].isnull().all(axis=1)\n",
    "n_missing = missing_mask.sum()\n",
    "\n",
    "print(f\"\\nMissing Data:\")\n",
    "print(f\"  Locations with all drivers missing: {n_missing}\")\n",
    "print(f\"  Percentage missing: {100 * n_missing / len(sos_coord_driver_df):.1f}%\")\n",
    "\n",
    "print(f\"\\nFirst few rows:\")\n",
    "sos_coord_driver_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge EOS drivers with coordinates\n",
    "eos_coord_driver_df = pd.merge(coords, eos_driver_df, on=\"location\", how=\"left\")\n",
    "\n",
    "print(\"EOS Data After Coordinate Merge:\")\n",
    "print(f\"  Total locations in coords: {len(coords)}\")\n",
    "print(f\"  Locations with EOS driver data: {len(eos_driver_df)}\")\n",
    "print(f\"  Final merged DataFrame: {eos_coord_driver_df.shape}\")\n",
    "\n",
    "missing_mask = eos_coord_driver_df[driver_cols].isnull().all(axis=1)\n",
    "n_missing = missing_mask.sum()\n",
    "\n",
    "print(f\"\\nMissing Data:\")\n",
    "print(f\"  Locations with all drivers missing: {n_missing}\")\n",
    "print(f\"  Percentage missing: {100 * n_missing / len(eos_coord_driver_df):.1f}%\")\n",
    "\n",
    "print(f\"\\nFirst few rows:\")\n",
    "eos_coord_driver_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Step 14: Spatial Imputation of Missing Values\n",
    "\n",
    "Fill missing driver values using nearby spatial locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_nearby_values(\n",
    "    df: pd.DataFrame,\n",
    "    lat_range: float = 0.5,\n",
    "    lon_range: float = 0.5,\n",
    "    max_distance: float = 2.0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Impute missing driver values using nearby spatial locations.\n",
    "    \"\"\"\n",
    "    driver_cols = [\n",
    "        \"hist_tmin\",\n",
    "        \"hist_tmax\",\n",
    "        \"hist_rad\",\n",
    "        \"hist_precip\",\n",
    "        \"hist_photo\",\n",
    "        \"hist_sm\",\n",
    "    ]\n",
    "\n",
    "    # Identify rows with missing data\n",
    "    missing_mask = df[driver_cols].isnull().all(axis=1)\n",
    "    n_missing = missing_mask.sum()\n",
    "\n",
    "    if n_missing == 0:\n",
    "        print(\"No missing values to impute\")\n",
    "        return df\n",
    "\n",
    "    print(f\"Found {n_missing} locations with missing data\")\n",
    "\n",
    "    # Create a copy to avoid modifying during iteration\n",
    "    df_result = df.copy()\n",
    "    filled_count = 0\n",
    "\n",
    "    # For each row with data, find and fill nearby missing rows\n",
    "    for idx, row in df.iterrows():\n",
    "        if row[driver_cols].isnull().any():\n",
    "            continue\n",
    "\n",
    "        # Find nearby locations with missing data\n",
    "        lat_match = (df[\"latitude\"] >= row[\"latitude\"] - lat_range) & (\n",
    "            df[\"latitude\"] <= row[\"latitude\"] + lat_range\n",
    "        )\n",
    "        lon_match = (df[\"longitude\"] >= row[\"longitude\"] - lon_range) & (\n",
    "            df[\"longitude\"] <= row[\"longitude\"] + lon_range\n",
    "        )\n",
    "\n",
    "        # Additional distance check\n",
    "        lat_diff = np.abs(df[\"latitude\"] - row[\"latitude\"])\n",
    "        lon_diff = np.abs(df[\"longitude\"] - row[\"longitude\"])\n",
    "        dist = np.sqrt(lat_diff**2 + lon_diff**2)\n",
    "        dist_match = dist <= max_distance\n",
    "\n",
    "        # Combined mask\n",
    "        nearby_missing = lat_match & lon_match & dist_match & missing_mask\n",
    "\n",
    "        if nearby_missing.any():\n",
    "            # Impute values\n",
    "            df_result.loc[nearby_missing, driver_cols] = row[driver_cols].values\n",
    "            filled_count += nearby_missing.sum()\n",
    "\n",
    "    # Count remaining missing values\n",
    "    n_remaining = df_result[driver_cols].isnull().all(axis=1).sum()\n",
    "    print(\n",
    "        f\"After imputation: {n_remaining} locations still missing ({n_missing - n_remaining} filled)\"\n",
    "    )\n",
    "\n",
    "    return df_result\n",
    "\n",
    "\n",
    "print(\"Imputing SOS missing values...\")\n",
    "sos_imputed_df = impute_nearby_values(sos_coord_driver_df)\n",
    "\n",
    "print(\"\\nImputing EOS missing values...\")\n",
    "# eos_imputed_df = impute_nearby_values(eos_coord_driver_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "## Step 15: Visualize Spatial Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize spatial coverage for SOS\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Before imputation\n",
    "has_data = ~sos_coord_driver_df[driver_cols].isnull().all(axis=1)\n",
    "axes[0].scatter(\n",
    "    sos_coord_driver_df.loc[has_data, \"longitude\"],\n",
    "    sos_coord_driver_df.loc[has_data, \"latitude\"],\n",
    "    c=\"green\",\n",
    "    s=10,\n",
    "    alpha=0.5,\n",
    "    label=\"Has data\",\n",
    ")\n",
    "axes[0].scatter(\n",
    "    sos_coord_driver_df.loc[~has_data, \"longitude\"],\n",
    "    sos_coord_driver_df.loc[~has_data, \"latitude\"],\n",
    "    c=\"red\",\n",
    "    s=10,\n",
    "    alpha=0.5,\n",
    "    label=\"Missing\",\n",
    ")\n",
    "axes[0].set_title(f\"SOS Before Imputation\\n{has_data.sum()} locations with data\")\n",
    "axes[0].set_xlabel(\"Longitude\")\n",
    "axes[0].set_ylabel(\"Latitude\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize spatial coverage for EOS\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Before imputation\n",
    "has_data = ~eos_coord_driver_df[driver_cols].isnull().all(axis=1)\n",
    "axes[0].scatter(\n",
    "    eos_coord_driver_df.loc[has_data, \"longitude\"],\n",
    "    eos_coord_driver_df.loc[has_data, \"latitude\"],\n",
    "    c=\"brown\",\n",
    "    s=10,\n",
    "    alpha=0.5,\n",
    "    label=\"Has data\",\n",
    ")\n",
    "axes[0].scatter(\n",
    "    eos_coord_driver_df.loc[~has_data, \"longitude\"],\n",
    "    eos_coord_driver_df.loc[~has_data, \"latitude\"],\n",
    "    c=\"red\",\n",
    "    s=10,\n",
    "    alpha=0.5,\n",
    "    label=\"Missing\",\n",
    ")\n",
    "axes[0].set_title(f\"EOS Before Imputation\\n{has_data.sum()} locations with data\")\n",
    "axes[0].set_xlabel(\"Longitude\")\n",
    "axes[0].set_ylabel(\"Latitude\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# After imputation\n",
    "has_data_imp = ~eos_imputed_df[driver_cols].isnull().all(axis=1)\n",
    "axes[1].scatter(\n",
    "    eos_imputed_df.loc[has_data_imp, \"longitude\"],\n",
    "    eos_imputed_df.loc[has_data_imp, \"latitude\"],\n",
    "    c=\"brown\",\n",
    "    s=10,\n",
    "    alpha=0.5,\n",
    "    label=\"Has data\",\n",
    ")\n",
    "axes[1].scatter(\n",
    "    eos_imputed_df.loc[~has_data_imp, \"longitude\"],\n",
    "    eos_imputed_df.loc[~has_data_imp, \"latitude\"],\n",
    "    c=\"red\",\n",
    "    s=10,\n",
    "    alpha=0.5,\n",
    "    label=\"Missing\",\n",
    ")\n",
    "axes[1].set_title(f\"EOS After Imputation\\n{has_data_imp.sum()} locations with data\")\n",
    "axes[1].set_xlabel(\"Longitude\")\n",
    "axes[1].set_ylabel(\"Latitude\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## Step 16: Final Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOS data quality\n",
    "n_complete_sos = (~sos_imputed_df[driver_cols].isnull().any(axis=1)).sum()\n",
    "print(\"SOS Data Quality:\")\n",
    "print(f\"  Total locations: {len(sos_imputed_df)}\")\n",
    "print(\n",
    "    f\"  Complete records: {n_complete_sos} ({100*n_complete_sos/len(sos_imputed_df):.1f}%)\"\n",
    ")\n",
    "print(f\"\\nMissing values per driver:\")\n",
    "for col in driver_cols:\n",
    "    n_missing = sos_imputed_df[col].isnull().sum()\n",
    "    print(f\"  {col}: {n_missing} ({100*n_missing/len(sos_imputed_df):.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "# EOS data quality\n",
    "n_complete_eos = (~eos_imputed_df[driver_cols].isnull().any(axis=1)).sum()\n",
    "print(\"EOS Data Quality:\")\n",
    "print(f\"  Total locations: {len(eos_imputed_df)}\")\n",
    "print(\n",
    "    f\"  Complete records: {n_complete_eos} ({100*n_complete_eos/len(eos_imputed_df):.1f}%)\"\n",
    ")\n",
    "print(f\"\\nMissing values per driver:\")\n",
    "for col in driver_cols:\n",
    "    n_missing = eos_imputed_df[col].isnull().sum()\n",
    "    print(f\"  {col}: {n_missing} ({100*n_missing/len(eos_imputed_df):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "## Step 17: Save Output Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save SOS driver data\n",
    "sos_output_path = f\"{OUTPUT_BASE}_SOS.csv\"\n",
    "sos_imputed_df.to_csv(sos_output_path, index=False)\n",
    "print(f\"✓ SOS driver data saved to: {sos_output_path}\")\n",
    "print(f\"  Shape: {sos_imputed_df.shape}\")\n",
    "\n",
    "# Save EOS driver data\n",
    "eos_output_path = f\"{OUTPUT_BASE}_EOS.csv\"\n",
    "eos_imputed_df.to_csv(eos_output_path, index=False)\n",
    "print(f\"\\n✓ EOS driver data saved to: {eos_output_path}\")\n",
    "print(f\"  Shape: {eos_imputed_df.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Driver data extraction complete!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook walked through the entire driver data extraction pipeline:\n",
    "\n",
    "1. **Data Loading**: Loaded test data, predictions, and coordinates\n",
    "2. **Data Preparation**: Created analysis DataFrame with temporal features\n",
    "3. **Phenology Detection**: Used CSIF slope analysis to identify SOS and EOS samples\n",
    "4. **Driver Extraction**: Extracted climate driver importance weights from attention scores\n",
    "5. **Spatial Imputation**: Filled missing values using nearby locations\n",
    "6. **Quality Checks**: Validated data completeness and spatial coverage\n",
    "7. **Output**: Saved driver data to CSV files for mapping\n",
    "\n",
    "The diagnostic outputs at each step help understand:\n",
    "- Data quality and completeness\n",
    "- Phenology detection performance\n",
    "- Driver weight distributions\n",
    "- Spatial coverage patterns\n",
    "- Impact of imputation strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
